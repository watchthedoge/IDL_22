{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dca00a9",
   "metadata": {},
   "source": [
    "# Task 2: \n",
    "## Sequence modelling with RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89164acf",
   "metadata": {},
   "source": [
    "Link to notebook: https://colab.research.google.com/drive/1itKfz1CLPuyPEjVDm0_f9-Y8QZ8bzAaV?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb3a362",
   "metadata": {},
   "source": [
    "We want to develop a neural network that learns how to add or subtract two integers that are at most two digits long. \n",
    "\n",
    "For example, given input strings of 5 characters: ‘81+24’ or ’41-89’ that consist of 2 two-digit long integers and an operand between them, the network should return a sequence of 3 characters: ‘105 ’ or ’-48 ’ that represent the result of their respective queries. \n",
    "\n",
    "Additionally, we want to build a model that generalizes well -> if the network can extract the underlying principles behind the ’+’ and ’-’ operands and associated operations, it should not need too many training examples to generate valid answers to unseen queries. To represent such queries we need 13 unique characters: 10 for digits (0-9), 2 for the ’+’ and ’-’ operands and one for whitespaces ’ ’ used as padding.\n",
    "\n",
    "The example above describes a text-to-text sequence mapping scenario. However, we can also use different modalities of data to represent our queries or answers. For that purpose, the MNIST handwritten digit dataset is going to be used again, however in a slightly different format. We can create an alternative image representation of our text queries described in the first paragraph by stacking multiple MNIST samples.\n",
    "\n",
    "We can then use these text and image queries/answers as our training data/labels and create different variations of mappings between text and images - e.g. using text input to predict image output or vice versa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef50b12d",
   "metadata": {},
   "source": [
    "### **Data stuff**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9521cb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca66787b",
   "metadata": {},
   "source": [
    "### **Task 1**: \n",
    "Analyze the code for generating numerical and image queries and their respective answers from MNIST data. Inspect the provided text-to-text RNN model and try to understand the dimensionality of the inputs and output tensors as well as how they are encoded/decoded (one-hot format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e24692d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79c9225c",
   "metadata": {},
   "source": [
    "### **Task 2**: \n",
    "In the text-to-text scenario, we are not interested in “memorizing” the whole addition/subtraction table (about 20,000 strings in total); instead, we want the network to learn some general principles\n",
    "behind these arithmetic operations. Therefore you should try multiple different splits for your training\n",
    "and test sets (50% train - 50% test; 25% train - 75% test, 10% train, 90% test etc.) and evaluate the\n",
    "accuracy and generalization capability of your models. Compare the outputs of your trained networks\n",
    "to the true labels, and find out what kind of mistakes your models make on the misclassified samples.\n",
    "Visualize these differences and try to explain them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1984afb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9195d9c9",
   "metadata": {},
   "source": [
    "### **Task 3**: \n",
    "reate an image-to-text RNN model: given a sequence of MNIST images that represent a query of an\n",
    "arithmetic operation, your model should return the answer in text format. Once you have trained your\n",
    "model, evaluate its accuracy and compare it to the text-to-text model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f787cab",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23a1e4df",
   "metadata": {},
   "source": [
    "### **Task 4**: \n",
    "Build a text-to-image RNN model: given a text query, your network should generate a sequence of images that represent the correct answer. In this case, it is harder to evaluate the performance of your 3 model quantitatively. However, you should provide examples of the output generated by your model\n",
    "in the report. What can you say about the appearance of these generated images?\n",
    "\n",
    "#### **Optional**: \n",
    "train a separate supervised model for evaluating the generated images of your text-to-image model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0416f2ec",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4735925e",
   "metadata": {},
   "source": [
    "### **Task 5**: \n",
    "Try adding additional LSTM layers to your encoder networks and see how the performance of your models changes. Try to explain these performance differences in the context of the mistakes that your network was making before. Tip: you should add a flag ”return sequences=True” to the first recurrent layer of your network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de54247f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
