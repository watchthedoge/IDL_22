{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8a901b-31f8-4847-bc41-ff31e8f74a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Task 2 for IDL, group 22\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "import matplotlib.pyplot as plt\n",
    "from tkinter.filedialog import askopenfilename\n",
    "\n",
    "class Data_options:\n",
    "\n",
    "    def __init__(self,dataset=\"Training\"):\n",
    "        self.dataset = dataset.lower()\n",
    "        if self.dataset != \"training\" and self.dataset != \"live\":\n",
    "            raise ValueError(\"Unknown data\")\n",
    "        data_path = \"../Data/\"\n",
    "        if self.dataset == \"training\":\n",
    "            self.train_x = pd.read_csv(data_path+\"train_in.csv\")\n",
    "            self.train_y = pd.read_csv(data_path+\"train_out.csv\")\n",
    "            self.test_x = pd.read_csv(data_path+\"test_in.csv\")\n",
    "            self.test_y = pd.read_csv(data_path+\"test_out.csv\")\n",
    "        else:\n",
    "            user_file = askopenfilename()\n",
    "            self.test_x = pd.read_csv(user_file)\n",
    "\n",
    "    def transform_to_image(self,input,output,\n",
    "                           prediction=None,prediction_probability=None,\n",
    "                           index=0):\n",
    "        \"\"\"\n",
    "        Input assumed to be flattened row by 256 columns\n",
    "        Default index is first in list, otherwise you can set it to any particular index you know of or \"Random\"\n",
    "        need both input and output to see what it should be\n",
    "        you can add the predicted category as well as its likelihood.\n",
    "        If a predicted value is given the output can be set to None\n",
    "        \"\"\"\n",
    "        if index.lower() == \"random\":\n",
    "            index = np.random.randint(input.shape[0])\n",
    "        row = input.iloc[index].values\n",
    "        image = row.reshape(16,16)\n",
    "        plt.imshow(image,cmap=\"gray\")\n",
    "        if prediction == None:\n",
    "            plt.title(f\"Number {output.iloc[index].values[0]}\")\n",
    "        else:\n",
    "            plt.title(f\"Predicted number {prediction.iloc[index].values[0]}, \"\n",
    "                      f\"at {prediction_probability.iloc[index].values[0]}% probability\")\n",
    "        plt.show()\n",
    "\n",
    "    def get_data(self):\n",
    "        if self.dataset == \"training\":\n",
    "            return {\"Train_in\" : self.train_x, \"Train_out\" : self.train_y, \"Test_in\" : self.test_x, \"Test_out\" : self.test_y}\n",
    "        else:\n",
    "            return {\"Test_in\": self.test_x}\n",
    "\n",
    "class Functions:\n",
    "\n",
    "    def softmax(self,x, temperature=1.0):\n",
    "        \"\"\"\n",
    "        Standard softmax function with temperature to adjust how certain the model is\n",
    "        temp < 1 means model will be more certain in its predictions\n",
    "        temp > 1 means model will be less certain in its predictions\n",
    "        the argmax function is unaffected by this\n",
    "        \"\"\"\n",
    "        e = np.array(x) / temperature\n",
    "        e -= e.max(axis=1, keepdims=True)\n",
    "        e = np.exp(e)\n",
    "        dist = e / np.sum(e, axis=1, keepdims=True)\n",
    "        return dist\n",
    "\n",
    "    def accuracy(self, predictions, true_classes):\n",
    "        matrix = np.c_[predictions,true_classes,np.zeros(len(predictions))]\n",
    "        mask = matrix[:,0] == matrix[:,1]\n",
    "        matrix[mask,2] = 1\n",
    "        accuracy = sum(matrix[:,2]==1)/len(matrix[:,2]) * 100\n",
    "        return round(accuracy,4)\n",
    "\n",
    "    def multiclass_cross_entropy_loss(self,scores,true_value):\n",
    "\n",
    "        \"\"\"\n",
    "        Negative log likelihood loss + episilon to avoid Nan issues ie. Log(0)\n",
    "        :param scores: Raw logits input transformed by softmax after\n",
    "        :param true_value: the true output y_i\n",
    "        :return: loss value\n",
    "        \"\"\"\n",
    "        num_classes = scores.shape[1]\n",
    "        num_samples = scores.shape[0]\n",
    "        scores = self.softmax(scores)\n",
    "        epsilon = 1e-10\n",
    "        NLL = -np.log(scores[np.arange(num_samples),true_value.flatten()]+epsilon)\n",
    "        loss = np.mean(NLL)\n",
    "        return round(loss,4)\n",
    "\n",
    "class Perceptron(object):\n",
    "\n",
    "    def __init__(self, learning_rate = 0.01, epochs = 50):\n",
    "        self.lr = learning_rate\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def relu(self,input):\n",
    "        \"\"\"\n",
    "        Went down a small rabbit hole and lost sight of the objective\n",
    "        relu activation for future use\n",
    "        \"\"\"\n",
    "        output = np.maximum(0,input)\n",
    "        return output\n",
    "\n",
    "    def weighted_sum(self,input):\n",
    "        \"\"\"\n",
    "        Matrix multiplication, essentially our prediction function y=mx+c I believe\n",
    "        \"\"\"\n",
    "        return np.dot(input,self.weights.T) + self.bias\n",
    "\n",
    "    def fit(self,X,y,verbose=True,update=\"gd\",weight_initializer=\"random\",bias_initializer=\"random\", **kwargs):\n",
    "        \"\"\"\n",
    "        This method creates our model and calls the prediction function and updates our model based on the prediction\n",
    "        The history is also stored and then returned in a dictionary\n",
    "        :param X: Input vector\n",
    "        :param y: true output class value\n",
    "        :param verbose: used to control the print statements\n",
    "        :param update: controls which update method is used, gradient descent or the standard perceptron rule\n",
    "        :param weight_initializer: Choose between initializers below, controls how weights are made\n",
    "        :param bias_initializer: Choose between initializers below, controls how biases are made\n",
    "        :param kwargs: allows more freedom for user\n",
    "        :return: history: the loss and accuracy vs epoch\n",
    "        \"\"\"\n",
    "        initializers = [\"random\", \"ones\", \"zeros\", \"normal\"]\n",
    "        # assert update rule is perceptron rule pr or gradient descent gd\n",
    "        if update != \"perception_rule\" and update != \"gradient_descent\":\n",
    "            raise ValueError(\"Pick update rule == gradient_descent or perception_rule\")\n",
    "        if weight_initializer not in initializers:\n",
    "            raise ValueError(\"Weight initializer is invalid\")\n",
    "        if bias_initializer not in initializers:\n",
    "            raise ValueError(\"Bias initializer is invalid\")\n",
    "        self.n_classes = int(y.max()) + 1\n",
    "        if weight_initializer == \"random\":\n",
    "            self.weights = np.random.rand(self.n_classes,X.shape[1])\n",
    "        elif weight_initializer == \"ones\":\n",
    "            self.weights = np.ones(shape=(self.n_classes,X.shape[1]))\n",
    "        elif weight_initializer == \"zeros\":\n",
    "            self.weights = np.zeros(shape=(self.n_classes,X.shape[1]))\n",
    "        elif weight_initializer == \"normal\":\n",
    "            self.weights = np.random.randn(self.n_classes,X.shape[1])\n",
    "        if bias_initializer == \"random\":\n",
    "            self.bias = np.random.rand(self.n_classes)\n",
    "        elif bias_initializer == \"ones\":\n",
    "            self.bias = np.ones(shape=(self.n_classes))\n",
    "        elif bias_initializer == \"zeros\":\n",
    "            self.bias = np.zeros(shape=(self.n_classes))\n",
    "        elif bias_initializer == \"normal\":\n",
    "            self.bias = np.random.randn(self.n_classes)\n",
    "\n",
    "        history={}\n",
    "        func=Functions()\n",
    "        train_loss = []\n",
    "        train_acc = []\n",
    "        for _ in range(self.epochs):\n",
    "            scores, predictions = self.predict(X)\n",
    "            scores = func.softmax(scores)\n",
    "            for xi, yi, score, y_pred in zip(X,y,scores,predictions):\n",
    "                if update == \"perception_rule\":\n",
    "                    if y_pred != yi:\n",
    "                        self.weights[yi] += self.lr * xi\n",
    "                        self.bias[yi] += self.lr\n",
    "                        self.weights[y_pred] -= self.lr * xi\n",
    "                        self.bias[y_pred] -= self.lr\n",
    "                elif update == \"gradient_descent\":\n",
    "                    #So this is what I think is the gradient descent from the book,\n",
    "                    # the score value - the certainty that it is that class is equal to the gradient? not sure but it\n",
    "                    # seems to work so maybe good ????\n",
    "                    self.weights[yi] -= self.lr*(score[yi]-1)*xi\n",
    "                    self.bias[yi] -= self.lr*(score[yi]-1)\n",
    "\n",
    "            loss = func.multiclass_cross_entropy_loss(scores,y)\n",
    "            accuracy = func.accuracy(predictions,y)\n",
    "            train_loss.append(loss)\n",
    "            train_acc.append(accuracy)\n",
    "            if verbose == True:\n",
    "                print(f\"Epoch {_} \\n\"\n",
    "                      f\"Accuracy: {accuracy}\\n\"\n",
    "                      f\"Loss: {loss}\")\n",
    "        history.update({\"Training Loss\": train_loss, \"Training Accuracy\": train_acc})\n",
    "        return history\n",
    "\n",
    "    def predict(self,input):\n",
    "        \"\"\"\n",
    "        If statement to handle training + test sets, there was a mismatch issue between them\n",
    "        :param input: X vector, ie flattened image containing the number\n",
    "        :return: scores, predicted class\n",
    "        \"\"\"\n",
    "        scores = self.weighted_sum(input)\n",
    "        if scores.ndim ==1:\n",
    "            return scores, (np.argmax(scores))\n",
    "        return scores, np.argmax(scores,axis=1)\n",
    "\n",
    "def plot(history,title=None):\n",
    "    \"\"\"\n",
    "    mostly just for the grid search\n",
    "    :return: saves a figure\n",
    "    \"\"\"\n",
    "    fig, ax1 = plt.subplots()\n",
    "    color = \"tab:blue\"\n",
    "    ax1.plot(history[\"Training Loss\"], color=color, label=\"Training Loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\", color=color)\n",
    "    ax1.tick_params(axis=\"y\", labelcolor=color)\n",
    "    color = \"tab:red\"\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(history[\"Training Accuracy\"], color=color, label=\"Training Accuracy\")\n",
    "    ax2.set_ylabel(\"Accuracy\", color=color)\n",
    "    ax2.tick_params(axis=\"y\", labelcolor=color)\n",
    "    fig.suptitle(title)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_multiple_histories(histories,title=None):\n",
    "    \"\"\"\n",
    "    For plotting the comparison graphs\n",
    "    :param histories: Histories should be a list containing dictionaries for each model,\n",
    "    these dictionaries should contain a key containing the label name\n",
    "\n",
    "    :return: saves fig\n",
    "    \"\"\"\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    #automatically setting up colors for loss/acc and avoiding whitewashed ones\n",
    "    n=len(histories)+int(ceil(1.25*len(histories)))\n",
    "    i=int(ceil(0.25*n))\n",
    "    cmap_blue = plt.get_cmap('Blues', n)\n",
    "    cmap_red = plt.get_cmap('Reds', n)\n",
    "\n",
    "    for history in histories:\n",
    "\n",
    "        if not history[\"Label\"]:\n",
    "            label=None\n",
    "        else:\n",
    "            label = history[\"Label\"]\n",
    "        #only need one condition here\n",
    "        if not history[\"Test Loss\"]:\n",
    "            test_loss = None\n",
    "            test_acc = None\n",
    "        else:\n",
    "            test_loss = history[\"Test Loss\"]\n",
    "            test_acc = history[\"Test Accuracy\"]\n",
    "\n",
    "        color = cmap_blue(i)\n",
    "        ax1.plot(history[\"Training Loss\"], color=color, label=label)\n",
    "        ax1.set_xlabel(\"Epochs\")\n",
    "        ax1.set_ylabel(\"Loss\", color=color)\n",
    "        ax1.tick_params(axis=\"y\", labelcolor=color)\n",
    "        ax1.hlines(y=test_loss,xmin=0,xmax=len(history[\"Training Loss\"]),color = color, linestyles= \"--\")\n",
    "\n",
    "        color = cmap_red(i)\n",
    "        ax2.plot(history[\"Training Accuracy\"], color=color, label=label)\n",
    "        ax2.set_ylabel(\"Accuracy\", color=color)\n",
    "        ax2.tick_params(axis=\"y\", labelcolor=color)\n",
    "        ax2.hlines(y=test_acc,xmin=0,xmax=len(history[\"Training Accuracy\"]),color = color, linestyles= \"-.\")\n",
    "\n",
    "        i+=1\n",
    "\n",
    "    fig.suptitle(title)\n",
    "    fig.legend()\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def main(verbose=False,grid_search=False):\n",
    "\n",
    "    func=Functions()\n",
    "    data_class=Data_options()\n",
    "    data = data_class.get_data()\n",
    "\n",
    "    # data_class.transform_to_image()\n",
    "\n",
    "    initializers=[\"random\",\"ones\",\"zeros\",\"normal\"]\n",
    "    updaters=[\"perception_rule\",\"gradient_descent\"]\n",
    "    learning_rates=[1,0.1,0.001,0.0001,0.00001]\n",
    "    epochs=[5,10,25,50,100,250,500]\n",
    "    number_of_runs = 10\n",
    "\n",
    "    X = data[\"Train_in\"].to_numpy(dtype=float)\n",
    "    y = data[\"Train_out\"].to_numpy(dtype=int)\n",
    "    test_x = data[\"Test_in\"].to_numpy(dtype=float)\n",
    "    test_y = data[\"Test_out\"].to_numpy(dtype=int)\n",
    "\n",
    "    #you can close this statement, this was just a curiosity of mine\n",
    "    if grid_search:\n",
    "        lowest_loss = np.inf\n",
    "        highest_accuracy = 0\n",
    "        best_params = None\n",
    "        for bias_init in initializers:\n",
    "            for weight_init in initializers:\n",
    "                for updater in updaters:\n",
    "                    for lr in learning_rates:\n",
    "                        for epoch in epochs:\n",
    "                            percep = Perceptron(learning_rate=lr,epochs=epoch)\n",
    "                            history = percep.fit(X, y,\n",
    "                                                 verbose=verbose,\n",
    "                                                 update=updater,\n",
    "                                                 weight_initializer = weight_init,\n",
    "                                                 bias_initializer = bias_init)\n",
    "                            predictions_batched = [percep.predict(x) for x in test_x]\n",
    "                            scores = np.array([r[0] for r in predictions_batched])\n",
    "                            y_pred = np.array([r[1] for r in predictions_batched])\n",
    "                            loss = func.multiclass_cross_entropy_loss(scores, test_y)\n",
    "                            accuracy = func.accuracy(y_pred, test_y)\n",
    "                            if verbose == True:\n",
    "                                print(f\"Test set : \\n\"\n",
    "                                      f\"Accuracy : {accuracy}\\n\"\n",
    "                                      f\"Loss : {loss}\")\n",
    "                            if loss < lowest_loss:\n",
    "                                lowest_loss = loss\n",
    "                                best_loss_params=(f\"learning rate = {lr}\"\n",
    "                                             f\"epochs = {epoch}\"\n",
    "                                             f\"updater = {updater}\"\n",
    "                                             f\"weight initializer = {weight_init}\"\n",
    "                                             f\"bias initializer = {bias_init}\")\n",
    "                            if accuracy > highest_accuracy:\n",
    "                                highest_accuracy = accuracy\n",
    "                                best_accuracy_params = (f\"learning rate = {lr}\"\n",
    "                                               f\"epochs = {epoch}\"\n",
    "                                               f\"updater = {updater}\"\n",
    "                                               f\"weight initializer = {weight_init}\"\n",
    "                                               f\"bias initializer = {bias_init}\")\n",
    "        print(f\"Lowest loss = {lowest_loss}\"\n",
    "              f\"for the following parameters\"\n",
    "              f\"{best_loss_params}\")\n",
    "        print(f\"highest accuracy = {highest_accuracy}\"\n",
    "              f\"for the following parameters\"\n",
    "              f\"{best_accuracy_params}\")\n",
    "\n",
    "    else:\n",
    "        histories=[]\n",
    "        for num_run in range(number_of_runs):\n",
    "            #best parameters\n",
    "            epoch=100\n",
    "            lr=0.001\n",
    "            weight_init=\"normal\"\n",
    "            bias_init = \"normal\"\n",
    "            updater = \"gradient_descent\"\n",
    "            label = f\"Run number {num_run}\"\n",
    "            percep = Perceptron(learning_rate=lr, epochs=epoch)\n",
    "            history = percep.fit(X=X, y=y,\n",
    "                                 verbose = verbose,\n",
    "                                 update=updater,\n",
    "                                 weight_initializer=weight_init,\n",
    "                                 bias_initializer=bias_init)\n",
    "            predictions_batched = [percep.predict(x) for x in test_x]\n",
    "            scores = np.array([r[0] for r in predictions_batched])\n",
    "            y_pred = np.array([r[1] for r in predictions_batched])\n",
    "            loss = func.multiclass_cross_entropy_loss(scores, test_y)\n",
    "            accuracy = func.accuracy(y_pred, test_y)\n",
    "            if verbose == True:\n",
    "                print(f\"Test set : \\n\"\n",
    "                      f\"Accuracy : {accuracy}\\n\"\n",
    "                      f\"Loss : {loss}\")\n",
    "\n",
    "            history.update({\"Label\":label, \"Test Loss\": loss, \"Test Accuracy\": accuracy})\n",
    "            histories.append(history)\n",
    "        plot_multiple_histories(histories,title=\"Comparison of multiple runs\")\n",
    "\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
